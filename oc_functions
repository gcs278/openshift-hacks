function oc_ingress_unmanaged_privileged() {
  oc_ingress_unmanaged
  oc_ingress_privileged
}

function oc_ingress_unmanaged_privileged_reset() {
  oc_ingress_managed
  oc_ingress_unprivileged
}

function oc_ingress_managed() {
  oc patch clusterversions/version --type=json --patch='[{"op":"add","path":"/spec/overrides","value":[{"kind":"Deployment","group":"apps/v1","name":"ingress-operator","namespace":"openshift-ingress-operator","unmanaged":false}]}]'
  oc_scale_cvo 1
  oc_ingress_scale_ingress_operator 1
  oc scale --replicas 2 -n openshift-ingress-operator ingresscontroller/default
}

function oc_ingress_unprivileged() {
  router=default
  if [[ "$1" != "" ]]; then
    router="$1"
  fi
  oc patch clusterrole/openshift-ingress-router --type=strategic --patch='{"rules":[{"apiGroups":[""],"resources":["endpoints","namespaces","services"],"verbs":["list","watch"]},{"apiGroups":["authentication.k8s.io"],"resources":["tokenreviews"],"verbs":["create"]},{"apiGroups":["authorization.k8s.io"],"resources":["subjectaccessreviews"],"verbs":["create"]},{"apiGroups":["route.openshift.io"],"resources":["routes"],"verbs":["list","watch"]},{"apiGroups":["route.openshift.io"],"resources":["routes/status"],"verbs":["update"]},{"apiGroups":["security.openshift.io"],"resourceNames":["hostnetwork"],"resources":["securitycontextconstraints"],"verbs":["use"]},{"apiGroups":["discovery.k8s.io"],"resources":["endpointslices"],"verbs":["list","watch"]}]}'
  oc patch -n openshift-ingress deployment/router-${router} --patch='{"spec":{"template":{"spec":{"securityContext":{"runAsUser":null}}}}}'
  oc patch -n openshift-ingress deployment/router-${router} --patch='{"spec":{"template":{"spec":{"hostNetwork":true}}}}'
}

function oc_ingress_unmanaged() {
  oc_scale_cvo 0
  oc_ingress_scale_ingress_operator 0

  oc patch clusterversions/version --type=json --patch='[{"op":"add","path":"/spec/overrides","value":[{"kind":"Deployment","group":"apps/v1","name":"ingress-operator","namespace":"openshift-ingress-operator","unmanaged":true}]}]'
  oc_ingress_scale_ingress_operator 0
}

function oc_ingress_privileged() {
  router=default
  if [[ "$1" != "" ]]; then
    router="$1"
  fi
  oc patch clusterrole/openshift-ingress-router --type=strategic --patch='{"rules":[{"apiGroups":[""],"resources":["endpoints","namespaces","services"],"verbs":["list","watch"]},{"apiGroups":["authentication.k8s.io"],"resources":["tokenreviews"],"verbs":["create"]},{"apiGroups":["authorization.k8s.io"],"resources":["subjectaccessreviews"],"verbs":["create"]},{"apiGroups":["route.openshift.io"],"resources":["routes"],"verbs":["list","watch"]},{"apiGroups":["route.openshift.io"],"resources":["routes/status"],"verbs":["update"]},{"apiGroups":["security.openshift.io"],"resourceNames":["privileged"],"resources":["securitycontextconstraints"],"verbs":["use"]},{"apiGroups":["discovery.k8s.io"],"resources":["endpointslices"],"verbs":["list","watch"]}]}'
  oc patch -n openshift-ingress deployment/router-${router} --patch='{"spec":{"template":{"spec":{"securityContext":{"runAsUser":0}}}}}'
}

function oc_scale_cvo() {
  if [[ $? == "" ]]; then
    echo "ERROR: you need to provide replicas"
    return 1
  fi
  oc scale --replicas $1 -n openshift-cluster-version deployments/cluster-version-operator
}

function oc_ingress_scale_router_default() {
  oc_ingress_scale_router default $1
}

function oc_ingress_scale_router() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide a router name"
    return 1
  fi
  if [[ $2 = "" ]]; then
    echo "ERROR: You need to provide a scale #"
    return 1
  fi
  oc scale --replicas=$2 -n openshift-ingress deployment/router-${1}
  oc -n openshift-ingress-operator patch ingresscontroller/$1 --type=merge --patch='{"spec":{"replicas":'${2}'}}'
}

function oc_ingress_scale_ingress_operator() {
  if [[ $1 = "" ]]; then
    echo "ERROR: You need to provide a scale #"
    return 1
  fi
  oc scale --replicas $1 -n openshift-ingress-operator deployments ingress-operator
}

function oc_ingress_rsh_only_router() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi

  if [[ $(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running |  wc -l) -ne 1 ]]; then
    echo "ERROR: There are either 0 or more than 1 ${router} routers running"
    oc get pods -n openshift-ingress
    return 1
  fi

  oc rsh -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-default" | grep Running | head -1 | awk '{print $1}')
}

function oc_ingress_rsh_any_router() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi
  oc rsh -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running | head -1 | awk '{print $1}')
}

function oc_ingress_exec_any_router() {
  router="$1"
  shift 1
  oc exec -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running | head -1 | awk '{print $1}') "$@"
}

function oc_ingress_cp_into_only_router_default() {
  if [[ $(oc get pods -n openshift-ingress --no-headers | grep "^router-default" | grep Running |  wc -l) -ne 1 ]]; then
    echo "ERROR: There are either 0 or more than 1 default routers running"
    oc get pods -n openshift-ingress
    return 1
  fi

  oc cp -n openshift-ingress $1 $(oc get pods -n openshift-ingress --no-headers | grep "^router-default" | grep Running | head -1 | awk '{print $1}'):${2}
}

function oc_ingress_cp_out_any_router_default() {
  oc cp -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-default" | grep Running | head -1 | awk '{print $1}'):${1} $(basename ${1})
}

function oc_ingress_haproxy_debug_router_default() {
  if [[ $(oc get pods -n openshift-ingress --no-headers | grep "^router-default" | grep Running |  wc -l) -ne 1 ]]; then
    echo "ERROR: There are either 0 or more than 1 default routers running"
    oc get pods -n openshift-ingress
    return 1
  fi

  oc exec -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-default" | grep Running | head -1 | awk '{print $1}') -- bash -c "killall haproxy; pkill haproxy; haproxy -f /var/lib/haproxy/conf/haproxy.config -V -d"
}


function oc_ingress_get_router_pods() {
  oc get pods -n openshift-ingress $@
}


function oc_ingress_get_router_deployments() {
  oc get deployments -n openshift-ingress $@
}

function oc_ingress_get_dnsrecords() {
  oc get dnsrecords -n openshift-ingress-operator $@
}

function oc_ingress_get_operator() {
  oc get pods -n openshift-ingress-operator $@
}

function oc_ingress_get_ingresscontrollers() {
  oc get -n openshift-ingress-operator ingresscontrollers $@
}

function oc_ingress_logs_operator() {
  oc logs -n openshift-ingress-operator -l name=ingress-operator
}

function oc_ingress_watch_ingresscontroller_progressing() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You must provide a ingresscontroller name"
    return 1
  fi
  watch -n1 --color 'oc get ingresscontroller -n openshift-ingress-operator '$1' -o jsonpath="{.status.conditions[?(@.type==\"Progressing\")]}" | yq -PC'
}

function oc_ingress_watch_ingresscontroller_upgradable() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You must provide a ingresscontroller name"
    return 1
  fi
  watch -n1 --color 'oc get ingresscontroller -n openshift-ingress-operator '$1' -o jsonpath="{.status.conditions[?(@.type==\"Upgradeable\")]}" | yq -PC'
}

function oc_ingress_watch_ingresscontroller_status_endpointPublishingStrategy() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You must provide a ingresscontroller name"
    return 1
  fi
  watch -n1 --color 'oc get -n openshift-ingress-operator ingresscontroller '$1' -o jsonpath="{.status.endpointPublishingStrategy}" | yq -PC'
}

function oc_dns_logs_operator() {
  oc logs -n openshift-dns-operator $(oc get -n openshift-dns-operator pods --no-headers | head -1 | awk '{print $1}') -c dns-operator $@
}

function oc_ingress_enable_http2_router_default() {
  oc -n openshift-ingress-operator annotate ingresscontrollers/default ingress.operator.openshift.io/default-enable-http2=true
}

function oc_ingress_edit_ingresscontroller() {
  if [[ "$1" == "" ]]; then
    IC="default"
  else
    IC="$1"
  fi

  oc edit -n openshift-ingress-operator ingresscontrollers/${IC}
}

function oc_ingress_edit_router_deployment() {
  if [[ "$1" == "" ]]; then
    IC="default"
  else
    IC="$1"
  fi

  oc edit -n openshift-ingress deployments/router-${IC}
}

function oc_ingress_edit_ingress_operator_deployment() {
  oc edit deployment -n openshift-ingress-operator ingress-operator
}

function oc_ingress_delete_ingresscontroller() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an ingress controller name"
    return 1
  fi

  oc delete -n openshift-ingress-operator ingresscontroller $@ &
  sleep 3
  oc_ingress_force_delete_terminating_router_pods
  wait 
}

function oc_ingress_logs_any_router_access() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi
  shift 1

  oc logs -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running | head -1 | awk '{print $1}') -c logs --follow $@
}

function oc_ingress_enable_access_logs() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide ingress controller name"
    return 1
  fi
  oc -n openshift-ingress-operator patch ingresscontroller/$1 --type=merge --patch='{"spec":{"logging":{"access":{"destination":{"type":"Container"}}}}}'
}

function oc_ingress_logs_any_router() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi
  shift 1
  
  oc logs -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running | head -1 | awk '{print $1}') --follow $@
}

function oc_ingress_logs_all_router() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi
  shift 1
  
  oc logs -n openshift-ingress -l "ingresscontroller.operator.openshift.io/deployment-ingresscontroller=${router}" $@
}

function oc_ingress_logs_any_router_wait_appear() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi
  shift 1
  
  while true; do
    pod=$(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running | head -1 | awk '{print $1}')
    if [[ "$pod" != "" ]]; then
      oc logs -n openshift-ingress $(oc get pods -n openshift-ingress --no-headers | grep "^router-${router}" | grep Running | head -1 | awk '{print $1}') --follow $@
    else
      echo -n "."
    fi
  done
}

function oc_ingress_run_logging_openshift_router() {
  oc -n openshift-ingress rsh -c router deploy/router-default sh -c 'rm -rf /tmp/haproxy && mkdir -p /tmp/haproxy/{router/{certs,cacerts,whitelists},{conf/.tmp,run,bin,log}} && /bin/openshift-router --v=10 --template=/var/lib/haproxy/conf/haproxy-config.template --reload=/bin/true --update-status=false --stats-port=1937 --metrics-type=haproxy --working-dir=/tmp/haproxy'
}

function oc_ingress_get_router_image() {
  oc_ingress_get_router_deployments router-default -o json | jq -r '.spec.template.spec.containers[0].image'
}

function oc_ingress_get_operator_image() {
  oc get deployment -n openshift-ingress-operator ingress-operator -o json | jq -r '.spec.template.spec.containers[0].image'
}

function oc_ingress_enable_verbose_openshift_router() {
  router=${1}
  if [[ "$router" == "" ]]; then
    router="default"
  fi
  oc -n openshift-ingress patch deploy/router-${router} --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","command":["/usr/bin/openshift-router","--v=10"]}]}}}}'
}


function oc_ingress_enable_verbose_openshift_router_image_local() {
  level=10
  if [[ "$3" != "" ]]; then
    level=$3
  fi
  podman build -t $2  -<<EOF
FROM $1
ENTRYPOINT ["/usr/bin/openshift-router","--v=${level}"]
EOF
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Build failed"
    return 1
  fi
}

function oc_ingress_patch_router_image() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an image"
    return 1
  fi
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","image":"'${1}'","imagePullPolicy":"Always"}]}}}}'
  oc -n openshift-ingress-operator patch deploy/ingress-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"ingress-operator","env":[{"name":"IMAGE","value":"'${1}'"}]}]}}}}'
  #oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router-default","imagePullPolicy":"Always"}]}}}}'
}

function oc_ingress_patch_operator_image() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an image"
    return 1
  fi
  # For later use, if we NEED the CVO, but just don't want it to manage ingress...
  #oc patch clusterversion version --type json -p '[{"op":"add","path":"/spec/overrides","value":[{"kind":"Deployment","group":"apps","name":"ingress-operator","namespace":"openshift-ingress-operator","unmanaged":true}]}]'
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  oc -n openshift-ingress-operator patch deploy/ingress-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"ingress-operator","image":"'${1}'"}]}}}}'
  oc -n openshift-ingress-operator patch deploy/ingress-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"ingress-operator","imagePullPolicy":"Always"}]}}}}'
}

function oc_ingress_force_delete_terminating_router_pods() {
  for i in $(oc get pods -n openshift-ingress --no-headers | grep Terminating | awk '{print $1}'); do
    oc delete -n openshift-ingress pod $i --force
  done
}

function oc_ingress_cleanup_old_router_pods() {
  hash=$(oc get deployment -n openshift-ingress router-default -o jsonpath={.spec.template.metadata.labels} | jq -r '.["ingresscontroller.operator.openshift.io/hash"]')
  for i in $(oc get pods -n openshift-ingress -o json | jq -r '.items[] | select(.metadata.labels."ingresscontroller.operator.openshift.io/hash"  != "6d58594c8b") | .metadata.name' | grep "router-"); do
    oc delete pod -n openshift-ingress $i --wait=false
  done
  oc_ingress_force_delete_terminating_router_pods
}

function oc_ingress_maxconn_auto() {
  oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"tuningOptions":{"maxConnections":-1}}}'
  oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"unsupportedConfigOverrides":{"maxConnections":-1}}}'
}

function oc_ingress_patch_maxconn() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You must provide a maxconn value"
    return 1
  fi
  oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"tuningOptions":{"maxConnections":'$1'}}}'
  oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"unsupportedConfigOverrides":{"maxConnections":'$1'}}}'
}


function oc_ingress_patch_ingresscontroller_routeselector() {
  if [[ "$1" == "" ]]; then
    oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"routeSelector":{"matchLabels":{"type":null}}}}'
  else
    oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge --patch='{"spec":{"routeSelector":{"matchLabels":{"type":"'$1'"}}}}'
  fi
}

function oc_ingress_rollout_router() {
  if [[ "$1" == "" ]]; then
    router="default"
  else
    router="$1"
  fi
  shift 1

  oc rollout restart deployment -n openshift-ingress router-${router}
}

function oc_ingress_rollout_ingressoperator() {
  oc rollout restart deployment -n openshift-ingress-operator ingress-operator
}

alias oc_update_pull_secrets="oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=/home/gspence/.secrets/pull-secret.txt"
alias oc_get_pull_secrets="oc get secret/pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}'"
alias oc_get_console_url="oc get routes -n openshift-console console -o go-template='{{.spec.host}}{{\"\n\"}}'"

function oc_open_console() {
  URL=$(oc_get_console_url)
  xdg-open https://${URL}
}

function oc_open_kibana() {
  URL=$(oc get route -n openshift-logging kibana -o go-template='{{.spec.host}}{{"\n"}}')
  echo $URL
  xdg-open https://${URL}
}

function oc_curl() {
  oc run curl-pod --image=radial/busyboxplus:curl -i --tty --rm
}

function oc_ingress_istioapi_logs() {
  oc logs -f $(oc get -n istio-system pod --no-headers | grep istio-ingressgateway | awk '{print $1}') -n istio-system
}

function oc_ingress_gwapi_logs() {
  oc logs -f $(oc get -n gwapi pod --no-headers | grep "gateway-" | awk '{print $1}') -n gwapi
}

function oc_ingress_domain() {
  oc get ingresses.config/cluster -o jsonpath={.spec.domain}
  echo
}

function oc_ingress_domain_base() {
  oc get dnses cluster -o jsonpath={.spec.baseDomain}
  echo
}

function oc_ingress_watch_router_conns() {
  echo "Run:"
  echo "oc_ingress_rsh_any_router"
  echo "watch -n.1 'echo \"show info\" | socat /var/lib/haproxy/run/haproxy.sock stdio'"
  #oc_ingress_exec_any_router default "watch -n .1 echo show info | socat /var/lib/haproxy/run/haproxy.sock stdio | grep CurrConns"
}

function oc_ingress_get_svc() {
  oc get svc -n openshift-ingress $@
}

function oc_ingress_patch_ingress_operator_local_binary() {
  #image=$(oc 
  podman pull --authfile /home/gspence/.secrets/pull-secret.txt 
}

function oc_get_image() {
  if [[ "$1" == "" ]]; then
    echo "Usage: oc_get_image <VERSION> <container>"
    return 1
  fi

  REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs quay.io/openshift-release-dev/ocp-release:${1}-x86_64
  if [[ $? -ne 0 ]]; then
    oc adm release info --pullspecs registry.ci.openshift.org/ocp/release:$1
    if [[ $? -ne 0 ]]; then
      # for shas
      oc adm release info --pullspecs registry.ci.openshift.org/ocp/release@$1
      if [[ $? -ne 0 ]]; then
        echo "Error running oc adm release info quay.io/openshift-release-dev/ocp-release:${1}-x86_64"
        echo "Error running oc adm release info registry.ci.openshift.org/ocp/release@$1"
        return 1
      fi
    fi
  fi
}

function oc_logs() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide a pod name"
    return 1
  fi
  pod_name="$1"
  shift 1
  for pod in $(oc get pods $@ --no-headers -o jsonpath='{range .items[*]}{@.metadata.name}{"\n"}' | grep "${pod_name}-"); do
    echo "${pod}:"
    oc logs $pod --timestamps $@
  done
}

function oc_dns_unmanaged() {
  oc_scale_cvo 0
  
  oc -n openshift-dns-operator scale deployment.apps/dns-operator --replicas=0
  oc patch clusterversions/version --type=json --patch='[{"op":"add","path":"/spec/overrides","value":[{"kind":"Deployment","group":"apps/v1","name":"dns-operator","namespace":"openshift-dns-operator","unmanaged":true}]}]'

  oc -n openshift-dns-operator scale deployment.apps/dns-operator --replicas=0
}

function oc_dns_managed() {
  oc_scale_cvo 1
  
  oc patch clusterversions/version --type=json --patch='[{"op":"add","path":"/spec/overrides","value":[{"kind":"Deployment","group":"apps/v1","name":"dns-operator","namespace":"openshift-dns-operator","unmanaged":false}]}]'

  oc -n openshift-dns-operator scale deployment.apps/dns-operator --replicas=1
}

function oc_dns_get_coredns_pods() {
  oc get -n openshift-dns pods $@
}

function oc_dns_get_operator_pods() {
  oc get -n openshift-dns-operator pods $@
}

function oc_dns_edit_configmap() {
  oc -n openshift-dns edit configmaps/dns-default $@
}

function oc_dns_edit_dnses_operator() {
  oc edit -n openshift-dns dns.operator/default $@
}

function oc_dns_edit_dnses_config() {
  oc edit -n openshift-dns dns.config/cluster $@
}

function oc_dns_edit_dnsdaemonset() {
  oc edit daemonsets.apps -n openshift-dns dns-default
}

function oc_dns_delete_all_coredns_pods() {
  for i in $(oc get pods -n openshift-dns --no-headers | awk '{print $1}' | grep -i dns-default); do
    echo "Deleting $i"
    oc delete --force -n openshift-dns pod $i
  done
}

function oc_ingress_clusterbuild_operator_via_make() {
  make buildconfig GIT_URL=$(git config --get remote.my-fork.url)
  make cluster-build DEPLOY=1
}

function oc_ingress_revert_operator_image() {
  img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep cluster-ingress-operator | awk '{print $2}')
  echo "Reverting ingress-operator image to $img"
  oc_ingress_patch_operator_image $img
}

# Takes version of cluster-ingress-operator, puts in the a new container, then runs it
# Alternative to debuging C-I-O locally
function oc_ingress_clusterbuild_operator(){
  tag=$(date +%s)
  if [[ "$1" != "" ]]; then 
    tag="$1"
  fi
  fix_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: fix_makefile_cgo failed"
    return 1
  fi
  make
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi
  revert_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: revert_makefile_cgo failed"
    return 1
  fi

  podman login quay.io
  img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep cluster-ingress-operator | awk '{print $2}')
  if [[ "$img" == "" ]]; then
    echo "ERROR: Couldn't get current cluster ingress operator image"
    return 1
  fi

  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi

  container=$(podman create $img)
  podman cp ~/src/github.com/openshift/cluster-ingress-operator/ingress-operator ${container}:/usr/bin/
  new_img="quay.io/${USER}/cluster-ingress-operator:${tag}"
  podman commit $container $new_img
  podman push $new_img

  oc_ingress_patch_operator_image $new_img
  echo "Patched cluster-ingress-operator to $new_img"
}

function oc_ccm_azure_clusterbuild_ccm(){
  tag=$(date +%s)
  if [[ "$1" != "" ]]; then 
    tag="$1"
  fi
  fix_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: fix_makefile_cgo failed"
    return 1
  fi
  make
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi
  revert_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: revert_makefile_cgo failed"
    return 1
  fi

  podman login quay.io
  img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep azure-cloud-controller-manager | awk '{print $2}')
  if [[ "$img" == "" ]]; then
    echo "ERROR: Couldn't get current azure-cloud-controller-manager image"
    return 1
  fi

  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi

  container=$(podman create $img)
  podman cp ~/src/github.com/openshift/cloud-provider-azure/ ${container}:/usr/bin/
  new_img="quay.io/${USER}/cluster-ingress-operator:${tag}"
  podman commit $container $new_img
  podman push $new_img

  oc_ingress_patch_operator_image $new_img
  echo "Patched cluster-ingress-operator to $new_img"
}

function fix_makefile_cgo() {
  if [[ ! -f Makefile ]]; then
    echo "Error: ./Makefile doesn't exist"
    return 1
  fi
  sed -i 's/CGO_ENABLED=1/CGO_ENABLED=0/g' Makefile
}

function revert_makefile_cgo() {
  if [[ ! -f Makefile ]]; then
    echo "Error: ./Makefile doesn't exist"
    return 1
  fi
  sed -i 's/CGO_ENABLED=0/CGO_ENABLED=1/g' Makefile
}

function oc_ingress_clusterbuild_router(){
  tag=$(date +%s)
  if [[ "$1" != "" ]]; then 
    tag="$1"
  fi
  fix_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: fix_makefile_cgo failed"
    return 1
  fi
  make
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi
  revert_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: revert_makefile_cgo failed"
    return 1
  fi

  podman login quay.io
  img="$2"
  if [[ "$img" == "" ]]; then
    img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep haproxy-router | awk '{print $2}')
    if [[ "$img" == "" ]]; then
      echo "ERROR: Couldn't get current cluster ingress operator image"
      return 1
    fi
  fi
  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi


  container=$(podman create $img)
  podman cp ~/src/github.com/openshift/router/openshift-router ${container}:/usr/bin/
  podman cp ~/src/github.com/openshift/router/images/router/haproxy/conf/haproxy-config.template ${container}:/var/lib/haproxy/conf
  new_img="quay.io/${USER}/router:${tag}"
  podman commit $container $new_img
  oc_ingress_enable_verbose_openshift_router_image_local $new_img $new_img
  for i in {1..10}; do
    podman push $new_img
    if [[ $? -eq 0 ]]; then
      break
    fi
    echo "Failed podman push $new_img...retrying"
  done

  oc_ingress_patch_router_image $new_img
  echo "Patched router to $new_img"
}


function oc_ingress_clusterbuild_router_ontop_existing_image(){
  tag=$(date +%s)
  if [[ "$1" != "" ]]; then 
    tag="$1"
  fi
 fix_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: fix_makefile_cgo failed"
    return 1
  fi
  make
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi
  revert_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: revert_makefile_cgo failed"
    return 1
  fi

  podman login quay.io
  img=$(oc_ingress_get_router_image)
  if [[ "$img" == "" ]]; then
    echo "ERROR: Couldn't get current cluster ingress operator image"
    return 1
  fi

  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi

  container=$(podman create $img)
  podman cp ~/src/github.com/openshift/router/openshift-router ${container}:/usr/bin/
  podman cp ~/src/github.com/openshift/router/images/router/haproxy/conf/haproxy-config.template ${container}:/var/lib/haproxy/conf
  new_img="quay.io/${USER}/router:${tag}"
  podman commit $container $new_img
  for i in {1..10}; do
    podman push $new_img
    if [[ $? -eq 0 ]]; then
      break
    fi
    echo "Failed podman push $new_img...retrying"
  done

  oc_ingress_patch_router_image $new_img
  echo "Patched router to $new_img"
}

function oc_dns_patch_operator_image() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an image"
    return 1
  fi
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  oc -n openshift-dns-operator patch deploy/dns-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"dns-operator","image":"'${1}'"}]}}}}'
  oc -n openshift-dns-operator patch deploy/dns-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"dns-operator","imagePullPolicy":"Always"}]}}}}'
}


# Takes version of cluster-dns-operator, puts in the a new container, then runs it
# Alternative to debuging C-D-O locally
function oc_dns_clusterbuild_operator(){
  if [[ "$1" == "" ]]; then 
    echo "ERROR: Provide a tag name for your image"
    return 1
  fi
  fix_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: fix_makefile_cgo failed"
    return 1
  fi
  make
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi
  revert_makefile_cgo
  if [[ $? -ne 0 ]]; then
    echo "ERROR: revert_makefile_cgo failed"
    return 1
  fi
  podman login quay.io
  img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep cluster-dns-operator | awk '{print $2}')
  if [[ "$img" == "" ]]; then
    echo "ERROR: Couldn't get current cluster dns operator image"
    return 1
  fi

  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi

  container=$(podman create $img)
  podman cp ~/src/github.com/openshift/cluster-dns-operator/dns-operator ${container}:/usr/bin/
  new_img="quay.io/${USER}/cluster-dns-operator:${1}"
  podman commit $container $new_img
  podman push $new_img

  oc_dns_patch_operator_image $new_img
  echo "Patched cluster-dns-operator to $new_img"
}

function oc_dns_get_operator_config {
  oc get dnses.operator.openshift.io $@
}

function oc_dns_edit_operator_config {
  oc edit dnses.operator.openshift.io default $@
}

function oc_dns_patch_coredns_image() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an image"
    return 1
  fi
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  oc -n openshift-dns patch daemonset/dns-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","image":"'${1}'","imagePullPolicy":"Always"}]}}}}'
  oc -n openshift-dns-operator patch deploy/dns-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"dns-operator","env":[{"name":"IMAGE","value":"'${1}'"}]}]}}}}'
}


function oc_ingress_exec_router_all {
  for i in $(oc get pods -n openshift-ingress -o json | jq -r '.items[].metadata.name'); do
    echo $i;
    oc exec -n openshift-ingress $i $@
  done
}


# GINKGO_EXTRA_ARGS="--focus=.*ingresscontroller*" make integration

function oc_apiserver_clusterbuild_apiserver(){
  tag=$(date +%s)
  if [[ "$1" != "" ]]; then 
    tag="$1"
  fi
  make
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi

  podman login quay.io
  img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep openshift-apiserver | awk '{print $2}')
  if [[ "$img" == "" ]]; then
    echo "ERROR: Couldn't get current openshift-apiserver image"
    return 1
  fi

  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi

  container=$(podman create $img)
  podman cp ~/src/github.com/openshift/openshift-apiserver/openshift-apiserver ${container}:/usr/bin/
  new_img="quay.io/${USER}/openshift-apiserver:${tag}"
  podman commit $container $new_img
  podman push $new_img

  oc_apiserver_patch_apiserver_image $new_img
  echo "Patched openshift-apiserver to $new_img"
}

function oc_apiserver_patch_apiserver_image() {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an image"
    return 1
  fi
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  oc -n openshift-apiserver patch deploy/apiserver --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"openshift-apiserver","image":"'${1}'","imagePullPolicy":"Always"}]}}}}'
  oc -n openshift-apiserver-operator patch deploy/openshift-apiserver-operator --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"openshift-apiserver-operator","env":[{"name":"IMAGE","value":"'${1}'"}]}]}}}}'
}

function oc_apiserver_managed() {
  oc patch clusterversions/version --type=json --patch='[{"op":"add","path":"/spec/overrides","value":[{"kind":"Deployment","group":"apps/v1","name":"openshift-apiserver-operator","namespace":"openshift-apiserver-operator","unmanaged":false}]}]'
  oc_scale_cvo 1 
  oc scale deployment/openshift-apiserver-operator -n openshift-apiserver-operator --replicas 1
}

function oc_apiserver_force_delete_terminating_apiserver_pods() {
  for i in $(oc get pods -n openshift-apiserver --no-headers | grep Terminating | awk '{print $1}'); do
    oc delete -n openshift-apiserver pod $i --force
  done
}

function oc_ingress_enable_dcm() {
  if [[ "$1" == "" ]]; then
    echo "error: you must provide a IC name"
    return 1
  fi
  oc patch deployment/router-default -n openshift-ingress --patch '{"spec": {"template": {"spec": {"containers": [{"name": "router", "args": ["--blueprint-route-pool-size=0"]}]}}}}'
  oc -n openshift-ingress-operator patch ingresscontroller/$1 --type=merge --patch='{"spec":{"unsupportedConfigOverrides":{"dynamicConfigManager":"true"}}}'
}


function oc_ingress_disable_dcm() {
  if [[ "$1" == "" ]]; then
    echo "error: you must provide a IC name"
    return 1
  fi
  oc -n openshift-ingress-operator patch ingresscontroller/$1 --type=merge --patch='{"spec":{"unsupportedConfigOverrides":{"dynamicConfigManager":null}}}'
}

function oc_ingress_enable_gatewayapi() {
  # Enable feature gate
  oc patch featuregates/cluster --type=merge --patch='{"spec":{"featureSet":"CustomNoUpgrade","customNoUpgrade":{"enabled":["GatewayAPI"]}}}'
  
  until oc get crd gatewayclasses.gateway.networking.k8s.io >/dev/null 2>&1; do
    echo "Waiting for GatewayClass CRD to be installed..."
    sleep 2
  done
  echo "GatewayClass CRD is now available."

  # Create GatewayClass to install Istio
  oc create -f -<<'EOF'
    apiVersion: gateway.networking.k8s.io/v1beta1
    kind: GatewayClass
    metadata:
      name: openshift-default
    spec:
      controllerName: openshift.io/gateway-controller
EOF
}

function find_and_copy_dlv_from_goland_installation() {
  # Took from https://github.com/frobware/router/blob/b80a6a209f2c195c34aaf2fa0c8e7ccb632b4a2b/hack/remote-debug/find-and-copy-dlv-from-goland-installation
  # This script automates the process of locating and copying the Delve
  # (dlv) debugger executable from a GoLand installation. The Dockerfile
  # in hack/remote-debug/Dockerfile expects to find a dlv executable in
  # the top-level directory.

  ide_home="$HOME/.local/share/JetBrains/Toolbox/apps/goland"

  if [[ ! -d "$ide_home" ]]; then
    echo "The GoLand IDE home directory does not exist at $ide_home." >&2
    return 1
  fi

  os_type=$(uname -s | tr '[:upper:]' '[:lower:]')
  arch_type=$(uname -m)

  # Set the architecture based on operating system and CPU type.
  case "$os_type-$arch_type" in
    "linux-x86_64"|"darwin-x86_64")
        arch="linux"; [ "$os_type" = "darwin" ] && arch="mac"
        ;;
    "linux-arm"*|"darwin-arm"*)
        arch="linuxarm"; [ "$os_type" = "darwin" ] && arch="macarm"
        ;;
    *)
        echo "Unsupported architecture: $arch_type" >&2
        return 1
        ;;
  esac

  source_dlv="$ide_home/plugins/go-plugin/lib/dlv/$arch/dlv"
  destination_dir="${1:-$(pwd)}"
  destination_dlv="$destination_dir/dlv"

  if [ -x "$source_dlv" ]; then
    cp -v "$source_dlv" "$destination_dlv"
  else
    echo "The executable does not exist at $source_dlv." >&2
    return 1
  fi
}

function oc_ccm_aws_remotedebug {
  oc_ccm_aws_unmanaged

  find_and_copy_dlv_from_goland_installation
  tag=$(date +%s)
  if [[ "$1" != "" ]]; then 
    tag="$1"
  fi
  VERSION=$(git describe --dirty --tags --match='v*')
  LD_FLAGS="-w -s -X k8s.io/component-base/version.gitVersion=${VERSION} -X main.gitVersion=${VERSION}"
  GO111MODULE=on CGO_ENABLED=1 go build \
    -trimpath \
    -ldflags="${LDFLAGS}" \
    -gcflags "all=-N -l" \
    -o=aws-cloud-controller-manager \
    cmd/aws-cloud-controller-manager/main.go
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Make failed"
    return 1
  fi

  podman login quay.io
  img=$(REGISTRY_AUTH_FILE=/home/gspence/.secrets/pull-secret.txt oc adm release info --pullspecs | grep aws-cloud-controller-manager | awk '{print $2}')
  if [[ "$img" == "" ]]; then
    echo "ERROR: Couldn't get current aws-cloud-controller-manager image"
    return 1
  fi

  podman pull --authfile /home/${USER}/.secrets/pull-secret.txt $img
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Failed to pull $img"
    return 1
  fi

  pwd
  new_img="quay.io/${USER}/aws-cloud-controller-manager:${tag}"
  cat > Dockerfile <<EOF
FROM $img
COPY aws-cloud-controller-manager /usr/bin/
COPY dlv /usr/bin
ENTRYPOINT ["dlv", "--listen=:7000", "--api-version=2", "--headless=true", "--accept-multiclient", "exec", "/bin/aws-cloud-controller-manager"]
EOF
  podman build -t $new_img -f Dockerfile .
  rm -f Dockerfile
  
  podman push $new_img

  oc_ccm_patch_aws_ccm $new_img
  echo "Patched AWS CCM to $new_img"

  oc_ccm_patch_aws_dlv_cmd

  oc_ccm_port_forward_dlv_aws
}

function oc_ccm_patch_aws_dlv_cmd {
  oc scale --replicas=1 deployment aws-cloud-controller-manager -n openshift-cloud-controller-manager
  oc patch deployment aws-cloud-controller-manager -n openshift-cloud-controller-manager --type=json -p='[
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/command",
    "value": [
      "/bin/bash",
      "-c",
      "#!/bin/bash\nset -o allexport\nif [[ -f /etc/kubernetes/apiserver-url.env ]]; then\n  source /etc/kubernetes/apiserver-url.env\nfi\nexec dlv --listen=:7000 --api-version=2 --headless=true --accept-multiclient exec /bin/aws-cloud-controller-manager -- \\\n--cloud-provider=aws \\\n--use-service-account-credentials=true \\\n--configure-cloud-routes=false \\\n--leader-elect=false \\\n-v=10\n"
    ]
  }
]'
}


function oc_ccm_aws_unmanaged {
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  oc scale -n openshift-cloud-controller-manager-operator deployments/cluster-cloud-controller-manager-operator --replicas 0
}

function oc_ccm_aws_managed {
  oc scale --replicas 1 -n openshift-cluster-version deployments/cluster-version-operator
  oc scale -n openshift-cloud-controller-manager-operator deployments/cluster-cloud-controller-manager-operator --replicas 1
}

function oc_ccm_patch_aws_ccm {
  if [[ "$1" == "" ]]; then
    echo "ERROR: You need to provide an image"
    return 1
  fi
  oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator
  
  oc patch -n openshift-cloud-controller-manager deployment/aws-cloud-controller-manager --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"cloud-controller-manager","image":"'${1}'","imagePullPolicy":"Always"}]}}}}'
  
  # If you want to patch while operator managed...
  #oc get configmap cloud-controller-manager-images -n openshift-cloud-controller-manager-operator -o json | jq '.data["images.json"] |= fromjson | .data["images.json"].cloudControllerManagerAWS = "'$1'" | .data["images.json"] |= tojson' | kubectl apply -f -
}

function oc_ccm_port_forward_dlv_aws {
  if [[ "$KUBECONFIG" == "" ]]; then
    echo "ERROR: KUBECONFIG needs to be set"
    return 1
  fi

  cat > /home/$USER/.config/systemd/user/oc-port-forward-delve.service <<EOF
[Unit]
Description=Persistent oc port-forward service for Delve debugger
After=network.target

[Service]
RestartSec=1
Restart=always
ExecStart=/usr/local/bin/oc --kubeconfig ${KUBECONFIG} port-forward -n openshift-cloud-controller-manager deployment/aws-cloud-controller-manager --address 127.0.0.1 7000:7000
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=default.target
EOF

  systemctl --user daemon-reload
  systemctl --user enable --now oc-port-forward-delve.service
  systemctl restart --user oc-port-forward-delve.service
}

function oc_ccm_get_pods {
  oc get pods -n openshift-cloud-controller-manager
}
