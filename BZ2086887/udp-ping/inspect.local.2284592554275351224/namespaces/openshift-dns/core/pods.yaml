---
apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.102.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.102.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:22:58Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-47lbt
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290890304"
    uid: 9b649140-fb1f-4108-89ae-a7f76ad5d3d5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-152.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9vgwg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-133-152.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-9vgwg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:23:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:23:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5a31f60ad3aa644e4842174b5deaeac05197c0dd620e3c78fa1492771b85a3f6
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:23:01Z"
    hostIP: 10.0.133.152
    phase: Running
    podIP: 10.130.102.2
    podIPs:
    - ip: 10.130.102.2
    qosClass: BestEffort
    startTime: "2022-06-21T20:22:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.248.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.248.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T12:14:07Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-578xx
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3289356825"
    uid: 87a3febd-3f6a-4719-b598-bebe234ca249
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-230.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gj7dd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-140-230.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-gj7dd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://82c8377f864a11b54fc1ce894fa86692a69cb402e1ee8e714dbb3444d94c08df
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:14:10Z"
    hostIP: 10.0.140.230
    phase: Running
    podIP: 10.130.248.3
    podIPs:
    - ip: 10.130.248.3
    qosClass: BestEffort
    startTime: "2022-06-21T12:14:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.104.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.104.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:42:43Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-5fsgd
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290955239"
    uid: 62fa1926-1161-4071-a4d7-50c105a61881
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-156-105.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b8kwx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-156-105.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-b8kwx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a6147817bbfce03619fe65e56ded2a862e1077a7fea42a85714a5e0e68424cdc
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:42:46Z"
    hostIP: 10.0.156.105
    phase: Running
    podIP: 10.130.104.3
    podIPs:
    - ip: 10.130.104.3
    qosClass: BestEffort
    startTime: "2022-06-21T20:42:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.98.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.98.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:20:31Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-76lmp
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290957271"
    uid: 6845745d-cc6b-4491-8011-c710df058b73
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-142-124.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nrmp5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-142-124.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-nrmp5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:43:43Z"
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5941d7c4aa306a1fcc98d8b97b538a9da7b733910c9fd0a77d34061b7a3476f2
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:20:33Z"
    hostIP: 10.0.142.124
    phase: Running
    podIP: 10.130.98.2
    podIPs:
    - ip: 10.130.98.2
    qosClass: BestEffort
    startTime: "2022-06-21T20:20:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.65.66"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.65.66"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T04:04:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-7ztww
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3261980367"
    uid: 04024fee-05b8-4689-a6e8-d250f68bb1f0
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-81.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ghhss
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-140-81.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-ghhss
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://1d635923f290981e72efe384ddf3f749e0241bac5c3bdaadff5987aa99c2d574
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:04:16Z"
    hostIP: 10.0.140.81
    phase: Running
    podIP: 10.130.65.66
    podIPs:
    - ip: 10.130.65.66
    qosClass: BestEffort
    startTime: "2022-06-14T04:04:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.104.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.104.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:34:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-8ftsq
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290929852"
    uid: 0bccefad-7ba5-4efa-a182-22af888babfb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-245.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-45qzf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-134-245.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-45qzf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://de56c0ba34c3578ea3a163cf2fa817bb4d410965bf502fa009d82bc8878534a3
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:34:19Z"
    hostIP: 10.0.134.245
    phase: Running
    podIP: 10.128.104.3
    podIPs:
    - ip: 10.128.104.3
    qosClass: BestEffort
    startTime: "2022-06-21T20:34:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.88.5"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.88.5"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:11:49Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-8jgrh
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290836814"
    uid: adefe2ed-e14b-4a4f-87c7-6ec5e03666ea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-57.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lsfhf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-130-57.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-lsfhf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://260fdce43c9c90de0b5ad2759c8a5e1163d06d3f8b0298a671f40c57fd3c10ce
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:11:52Z"
    hostIP: 10.0.130.57
    phase: Running
    podIP: 10.128.88.5
    podIPs:
    - ip: 10.128.88.5
    qosClass: BestEffort
    startTime: "2022-06-21T20:11:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.1.159"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.1.159"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T04:04:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-d7tx5
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3261980360"
    uid: 737b1f53-d4ba-41e8-8639-dc9a5061c608
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-175-171.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9fs45
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-175-171.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-9fs45
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://62c361fb9629bd1d06356150095de5afd274bb1c69cf658c1765dbbfcd70b2b5
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:04:15Z"
    hostIP: 10.0.175.171
    phase: Running
    podIP: 10.128.1.159
    podIPs:
    - ip: 10.128.1.159
    qosClass: BestEffort
    startTime: "2022-06-14T04:04:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.13.243"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.13.243"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T04:04:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-ddtj7
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3261980377"
    uid: c248727b-21a0-4f7f-b041-55a618c66962
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-124.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zh4vm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-145-124.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-zh4vm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://71624430608b570f5296979920ab32d9f05946edd086ba7acbafdec8f84e7a71
      image: image-registry.openshift-image-registry.svc:5000/ci-op-pg51z8nj/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: image-registry.openshift-image-registry.svc:5000/ci-ln-01ixcnt/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:04:16Z"
    hostIP: 10.0.145.124
    phase: Running
    podIP: 10.128.13.243
    podIPs:
    - ip: 10.128.13.243
    qosClass: BestEffort
    startTime: "2022-06-14T04:04:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.52.5"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.52.5"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T17:24:53Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-ffxzf
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290296180"
    uid: 942566ea-8746-4a08-a610-675a4bb30768
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-15.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lklr9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-145-15.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-lklr9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8d98557379ee49bb121455a2c8c5a3bdc1f8559a83984233b5f70fad12967668
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:24:58Z"
    hostIP: 10.0.145.15
    phase: Running
    podIP: 10.129.52.5
    podIPs:
    - ip: 10.129.52.5
    qosClass: BestEffort
    startTime: "2022-06-21T17:24:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.60.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.60.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T17:33:15Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-h58ht
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290339899"
    uid: 3e6d25d8-c69f-4c02-bdc9-6d20e6260ec2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-143.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zbdlq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-134-143.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-zbdlq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://2d8e804c1d1338c8ce575840f98a96b0954e55f07d20dd1c9dbd7cb7d5e6bc10
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:33:18Z"
    hostIP: 10.0.134.143
    phase: Running
    podIP: 10.131.60.3
    podIPs:
    - ip: 10.131.60.3
    qosClass: BestEffort
    startTime: "2022-06-21T17:33:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.102.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.102.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:22:52Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-j4wbl
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290889816"
    uid: f85add60-540f-402f-bb40-01746042a61e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-3.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bv6v4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-130-3.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-bv6v4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ec14fea46e1c3ef2880f46260c3e0c87328b9da17a60089685000cf3f229458e
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:22:55Z"
    hostIP: 10.0.130.3
    phase: Running
    podIP: 10.128.102.2
    podIPs:
    - ip: 10.128.102.2
    qosClass: BestEffort
    startTime: "2022-06-21T20:22:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.64.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.64.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T17:40:19Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-kg7cb
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290365344"
    uid: 5e8534e9-84f1-42af-aef6-3c81a53cf7ea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-129-92.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lhltf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-129-92.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-lhltf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://eaa7d1f2f9084c4b1fb9466fe1c61eaf95fe2e126ef518be12157484d4a45b36
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:40:21Z"
    hostIP: 10.0.129.92
    phase: Running
    podIP: 10.128.64.3
    podIPs:
    - ip: 10.128.64.3
    qosClass: BestEffort
    startTime: "2022-06-21T17:40:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.98.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.98.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:15:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-kgkg8
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290858141"
    uid: 7cd0ac2c-f535-4472-affe-2713bdc89575
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-147-79.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j7789
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-147-79.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-j7789
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ff73ca7b2bfb55a56d026228a9621316d7909b53983f8ccf636d7f5d45a40c43
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:15:17Z"
    hostIP: 10.0.147.79
    phase: Running
    podIP: 10.129.98.2
    podIPs:
    - ip: 10.129.98.2
    qosClass: BestEffort
    startTime: "2022-06-21T20:15:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.5.134"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.5.134"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T04:04:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-lwnds
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3261980402"
    uid: ef0ceb3f-1e67-4879-915b-78e049b2a875
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-231.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tv45w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-133-231.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-tv45w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ed6c84a11a9bb1b9eebb9188aabf1e0f9f91a2fceeaa744cfb2996733248c967
      image: image-registry.openshift-image-registry.svc:5000/ci-op-0d22fy3k/stable-latest-cvp-common-claim@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: image-registry.openshift-image-registry.svc:5000/ci-op-0d22fy3k/stable-latest-cvp-common-claim@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:04:16Z"
    hostIP: 10.0.133.231
    phase: Running
    podIP: 10.129.5.134
    podIPs:
    - ip: 10.129.5.134
    qosClass: BestEffort
    startTime: "2022-06-14T04:04:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.44.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.44.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T16:58:12Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-m5zgp
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290198236"
    uid: 37f48323-3bd1-4551-b6cf-2abe20346110
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-136-253.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2thcf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-136-253.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-2thcf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a3788dbf643a54b244ffa471f1281fa90a4170698e67f67ca631076bc5be334c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T16:58:15Z"
    hostIP: 10.0.136.253
    phase: Running
    podIP: 10.130.44.3
    podIPs:
    - ip: 10.130.44.3
    qosClass: BestEffort
    startTime: "2022-06-21T16:58:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.98.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.98.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:20:31Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-mndtd
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290879922"
    uid: 5563167c-5bc3-43ae-98b3-f1f520dcbe8d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-141-152.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qzbl4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-141-152.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-qzbl4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://4e98033355c307e8ed8047a69866edaacbbaf934f0eb21a35eb47d8bfc021f51
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:20:34Z"
    hostIP: 10.0.141.152
    phase: Running
    podIP: 10.131.98.3
    podIPs:
    - ip: 10.131.98.3
    qosClass: BestEffort
    startTime: "2022-06-21T20:20:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.100.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.100.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:21:08Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-p7hcq
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290883072"
    uid: bda03b0a-6255-4b21-a5e3-6c91f83c690f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-110.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nxvr2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-130-110.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-nxvr2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://408c8fdecc77bec317fa8c88ab4dab9086554b9aa5ee5b0a03d0ab5ba8875e88
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:21:10Z"
    hostIP: 10.0.130.110
    phase: Running
    podIP: 10.129.100.2
    podIPs:
    - ip: 10.129.100.2
    qosClass: BestEffort
    startTime: "2022-06-21T20:21:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.52.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.52.3"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T17:27:18Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-r7b7d
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290307780"
    uid: cdf89e3e-500f-4d0b-a8df-79c73a14a718
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-150-201.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hglbb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-150-201.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-hglbb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://9eb6e398192e4c8651770c32423c28d23e37f980cf61a110d89eeee3cafc8a4e
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:27:21Z"
    hostIP: 10.0.150.201
    phase: Running
    podIP: 10.130.52.3
    podIPs:
    - ip: 10.130.52.3
    qosClass: BestEffort
    startTime: "2022-06-21T17:27:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.226.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.226.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T06:09:52Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-rjhsf
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3288549066"
    uid: 22f02017-7c8e-4062-9713-d19fe698c54e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-249.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lqzbb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-145-249.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-lqzbb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ee632a9d95afc55eaea28c5a559154fab809da1d6297c7916ffb9337cc0e6006
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T06:09:55Z"
    hostIP: 10.0.145.249
    phase: Running
    podIP: 10.129.226.2
    podIPs:
    - ip: 10.129.226.2
    qosClass: BestEffort
    startTime: "2022-06-21T06:09:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.1.100"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.1.100"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T04:04:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-s4k6v
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3261980381"
    uid: 86d20718-e105-4638-9c7c-9d236cff41ec
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-159-123.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7m2r2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-159-123.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-7m2r2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://fdb44f2bca8c58bcf9da4f713c4d7507c0304dc175200e142023f036e309138e
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:04:16Z"
    hostIP: 10.0.159.123
    phase: Running
    podIP: 10.129.1.100
    podIPs:
    - ip: 10.129.1.100
    qosClass: BestEffort
    startTime: "2022-06-14T04:04:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.171.89"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.171.89"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T04:04:14Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-tr9b2
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3261980382"
    uid: 7a6a24af-5b5a-410b-9d8f-7691cd3c697a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-169-113.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vb6m6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-169-113.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-vb6m6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:04:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://41613207e9344495a9c6711896955b26330da97c47865282cbf70a33eac40cf5
      image: image-registry.openshift-image-registry.svc:5000/ci-ln-6wz80kk/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: image-registry.openshift-image-registry.svc:5000/ci-ln-6wz80kk/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:04:16Z"
    hostIP: 10.0.169.113
    phase: Running
    podIP: 10.131.171.89
    podIPs:
    - ip: 10.131.171.89
    qosClass: BestEffort
    startTime: "2022-06-14T04:04:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.104.4"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.104.4"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:37:37Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-vwwxn
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290938324"
    uid: 4353b821-3e3d-4228-a65f-935d0f1483f6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-149-140.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ltdr2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-149-140.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-ltdr2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://063213379c76647b69ae2b93d71203bcdc1ffa180c2cb329103e67efbe007580
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:37:39Z"
    hostIP: 10.0.149.140
    phase: Running
    podIP: 10.129.104.4
    podIPs:
    - ip: 10.129.104.4
    qosClass: BestEffort
    startTime: "2022-06-21T20:37:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.80.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.80.2"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-21T20:08:26Z"
    generateName: curler-
    labels:
      app: curler
      controller-revision-hash: 6d64dcc584
      pod-template-generation: "1"
    name: curler-zs2l4
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: curler
      uid: e99a1b64-2717-4c7b-ad22-a59fac93efc3
    resourceVersion: "3290957603"
    uid: dd2e7c90-b8f9-4861-bec5-2fd36b94bb58
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-139-131.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -uo pipefail
        while :
          do curl -skw "dnslookup: %{time_namelookup} | connect: %{time_connect} | appconnect: %{time_appconnect} | pretransfer: %{time_pretransfer} | starttransfer: %{time_starttransfer} | total: %{time_total} | size: %{size_download}\n" "https://canary-openshift-ingress-canary.${CLUSTER_INGRESS_DOMAIN}/"
          sleep 1
          done
      env:
      - name: CLUSTER_INGRESS_DOMAIN
        value: apps.build01.ci.devcluster.openshift.com
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: curl
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8ljjj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-139-131.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-8ljjj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:43:48Z"
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://0d8efbd41098fa6d265c6c10f04061045b2b090665278f13ee9d03faad241433
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: curl
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:08:28Z"
    hostIP: 10.0.139.131
    phase: Running
    podIP: 10.131.80.2
    podIPs:
    - ip: 10.131.80.2
    qosClass: BestEffort
    startTime: "2022-06-21T20:08:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.1.232"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.1.232"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T13:59:23Z"
    generateName: dns-default-
    labels:
      controller-revision-hash: 57d85f4d55
      dns.operator.openshift.io/daemonset-dns: default
      pod-template-generation: "74"
    name: dns-default-6kjct
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dns-default
      uid: 0b15ce96-eb69-42f3-9dc5-d2a5e440e3bc
    resourceVersion: "3263818136"
    uid: 8220baae-6523-4380-bf24-d33d0dcf34fb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-175-171.ec2.internal
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      command:
      - coredns
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dns
      ports:
      - containerPort: 5353
        name: dns
        protocol: UDP
      - containerPort: 5353
        name: dns-tcp
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nn4j8
        readOnly: true
    - args:
      - --logtostderr
      - --secure-listen-address=:9154
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      - --upstream=http://127.0.0.1:9153/
      - --tls-cert-file=/etc/tls/private/tls.crt
      - --tls-private-key-file=/etc/tls/private/tls.key
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9154
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 40Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls/private
        name: metrics-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nn4j8
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    imagePullSecrets:
    - name: dns-dockercfg-6whsj
    nodeName: ip-10-0-175-171.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dns
    serviceAccountName: dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: dns-default
      name: config-volume
    - name: metrics-tls
      secret:
        defaultMode: 420
        secretName: dns-default-metrics-tls
    - name: kube-api-access-nn4j8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T13:59:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T13:59:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T13:59:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T13:59:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://08c8d3751cfd29036f7252a5643f01ad3ebb4804b7e1a37eb46a8f50f3fe712a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      lastState: {}
      name: dns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T13:59:25Z"
    - containerID: cri-o://bfdd554278775c07b7b899ba233243995c3ec281d2041559a0a330413a2a83a8
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T13:59:25Z"
    hostIP: 10.0.175.171
    phase: Running
    podIP: 10.128.1.232
    podIPs:
    - ip: 10.128.1.232
    qosClass: Burstable
    startTime: "2022-06-14T13:59:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.1.172"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.1.172"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T14:04:05Z"
    generateName: dns-default-
    labels:
      controller-revision-hash: 57d85f4d55
      dns.operator.openshift.io/daemonset-dns: default
      pod-template-generation: "74"
    name: dns-default-6p6vb
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dns-default
      uid: 0b15ce96-eb69-42f3-9dc5-d2a5e440e3bc
    resourceVersion: "3263840714"
    uid: 2b92a0f0-1027-47fd-a169-35f947572530
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-159-123.ec2.internal
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      command:
      - coredns
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dns
      ports:
      - containerPort: 5353
        name: dns
        protocol: UDP
      - containerPort: 5353
        name: dns-tcp
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kh8mv
        readOnly: true
    - args:
      - --logtostderr
      - --secure-listen-address=:9154
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      - --upstream=http://127.0.0.1:9153/
      - --tls-cert-file=/etc/tls/private/tls.crt
      - --tls-private-key-file=/etc/tls/private/tls.key
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9154
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 40Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls/private
        name: metrics-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kh8mv
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    imagePullSecrets:
    - name: dns-dockercfg-6whsj
    nodeName: ip-10-0-159-123.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dns
    serviceAccountName: dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: dns-default
      name: config-volume
    - name: metrics-tls
      secret:
        defaultMode: 420
        secretName: dns-default-metrics-tls
    - name: kube-api-access-kh8mv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:04:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:04:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:04:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:04:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ab7c6152aa9a953e42c8060b95d6bca7d88040705eed48d9d32679f9b0a8418f
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      lastState: {}
      name: dns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T14:04:08Z"
    - containerID: cri-o://2035c49eb21443c922b21f597c0187ed4f09418970126e110f20aa7f7c8a2297
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T14:04:08Z"
    hostIP: 10.0.159.123
    phase: Running
    podIP: 10.129.1.172
    podIPs:
    - ip: 10.129.1.172
    qosClass: Burstable
    startTime: "2022-06-14T14:04:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.5.157"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.5.157"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T14:02:03Z"
    generateName: dns-default-
    labels:
      controller-revision-hash: 57d85f4d55
      dns.operator.openshift.io/daemonset-dns: default
      pod-template-generation: "74"
    name: dns-default-b6d74
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dns-default
      uid: 0b15ce96-eb69-42f3-9dc5-d2a5e440e3bc
    resourceVersion: "3263829061"
    uid: a34548c5-6e35-4085-9dd4-0e2fd7b9b3ce
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-231.ec2.internal
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      command:
      - coredns
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dns
      ports:
      - containerPort: 5353
        name: dns
        protocol: UDP
      - containerPort: 5353
        name: dns-tcp
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xcth8
        readOnly: true
    - args:
      - --logtostderr
      - --secure-listen-address=:9154
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      - --upstream=http://127.0.0.1:9153/
      - --tls-cert-file=/etc/tls/private/tls.crt
      - --tls-private-key-file=/etc/tls/private/tls.key
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9154
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 40Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls/private
        name: metrics-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xcth8
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    imagePullSecrets:
    - name: dns-dockercfg-6whsj
    nodeName: ip-10-0-133-231.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dns
    serviceAccountName: dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: dns-default
      name: config-volume
    - name: metrics-tls
      secret:
        defaultMode: 420
        secretName: dns-default-metrics-tls
    - name: kube-api-access-xcth8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:02:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:02:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:02:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:02:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://af60dac70c95680cc34ac094de0908d6295bd50d255dd48a5434a41fd550fcd7
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      lastState: {}
      name: dns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T14:02:06Z"
    - containerID: cri-o://ff3598e12198e3aad4d1e04aeae29d6e3589cab0b6aa873d38fceb00651fbab5
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T14:02:06Z"
    hostIP: 10.0.133.231
    phase: Running
    podIP: 10.129.5.157
    podIPs:
    - ip: 10.129.5.157
    qosClass: Burstable
    startTime: "2022-06-14T14:02:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.171.110"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.171.110"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T13:58:08Z"
    generateName: dns-default-
    labels:
      controller-revision-hash: 57d85f4d55
      dns.operator.openshift.io/daemonset-dns: default
      pod-template-generation: "74"
    name: dns-default-cl797
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dns-default
      uid: 0b15ce96-eb69-42f3-9dc5-d2a5e440e3bc
    resourceVersion: "3289818468"
    uid: 84377567-9112-481a-8ed4-c0c51964dec3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-169-113.ec2.internal
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      command:
      - coredns
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dns
      ports:
      - containerPort: 5353
        name: dns
        protocol: UDP
      - containerPort: 5353
        name: dns-tcp
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vgqqh
        readOnly: true
    - args:
      - --logtostderr
      - --secure-listen-address=:9154
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      - --upstream=http://127.0.0.1:9153/
      - --tls-cert-file=/etc/tls/private/tls.crt
      - --tls-private-key-file=/etc/tls/private/tls.key
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9154
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 40Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls/private
        name: metrics-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vgqqh
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    imagePullSecrets:
    - name: dns-dockercfg-6whsj
    nodeName: ip-10-0-169-113.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dns
    serviceAccountName: dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: dns-default
      name: config-volume
    - name: metrics-tls
      secret:
        defaultMode: 420
        secretName: dns-default-metrics-tls
    - name: kube-api-access-vgqqh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T13:58:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T14:42:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T14:42:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T13:58:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ed023c33eb1c69dfb10cf6d1ece90035f53a1f18700d62f4086dd7b3684827e5
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      lastState: {}
      name: dns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T13:58:10Z"
    - containerID: cri-o://8b267098cf999174cfdfbe1c1ed24718895bbad92b7cce8548208143f246a420
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T13:58:10Z"
    hostIP: 10.0.169.113
    phase: Running
    podIP: 10.131.171.110
    podIPs:
    - ip: 10.131.171.110
    qosClass: Burstable
    startTime: "2022-06-14T13:58:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.65.131"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.65.131"
            ],
            "default": true,
            "dns": {}
        }]
    creationTimestamp: "2022-06-14T14:09:12Z"
    generateName: dns-default-
    labels:
      controller-revision-hash: 57d85f4d55
      dns.operator.openshift.io/daemonset-dns: default
      pod-template-generation: "74"
    name: dns-default-pfbgr
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dns-default
      uid: 0b15ce96-eb69-42f3-9dc5-d2a5e440e3bc
    resourceVersion: "3263875595"
    uid: d1889865-b4b4-4802-92aa-80d44d517d76
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-81.ec2.internal
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      command:
      - coredns
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dns
      ports:
      - containerPort: 5353
        name: dns
        protocol: UDP
      - containerPort: 5353
        name: dns-tcp
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-brmtf
        readOnly: true
    - args:
      - --logtostderr
      - --secure-listen-address=:9154
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      - --upstream=http://127.0.0.1:9153/
      - --tls-cert-file=/etc/tls/private/tls.crt
      - --tls-private-key-file=/etc/tls/private/tls.key
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9154
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 40Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls/private
        name: metrics-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-brmtf
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    imagePullSecrets:
    - name: dns-dockercfg-6whsj
    nodeName: ip-10-0-140-81.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dns
    serviceAccountName: dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: dns-default
      name: config-volume
    - name: metrics-tls
      secret:
        defaultMode: 420
        secretName: dns-default-metrics-tls
    - name: kube-api-access-brmtf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:09:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:09:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:09:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T14:09:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a377ad192f47b787d3a90acdd94f52ea63d838220fcebd745358a5e9a6c96475
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2b423e88cdd37f307aff51cbb0f53fc45deff9618f5b4f12bfb78bea7aff51a2
      lastState: {}
      name: dns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T14:09:15Z"
    - containerID: cri-o://42b98e360f9612e0752705dcd983afc8b334ebbae0a0114407cd14378b89f655
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3df935837634adb5df8080ac7263c7fb4c4f9d8fd45b36e32ca4fb802bdeaecc
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T14:09:15Z"
    hostIP: 10.0.140.81
    phase: Running
    podIP: 10.130.65.131
    podIPs:
    - ip: 10.130.65.131
    qosClass: Burstable
    startTime: "2022-06-14T14:09:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-14T04:30:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-2mczg
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3262035291"
    uid: aab615bd-7d13-4888-9393-95a346952fbe
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-124.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kvvhb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-145-124.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-kvvhb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://d886e383521c06c8f554d06f51f1d234bf730248ac0c17f2fbcad92684b90122
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: image-registry.openshift-image-registry.svc:5000/ci-op-rl3rc47g/stable@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:30:53Z"
    hostIP: 10.0.145.124
    phase: Running
    podIP: 10.0.145.124
    podIPs:
    - ip: 10.0.145.124
    qosClass: BestEffort
    startTime: "2022-06-14T04:30:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:24:53Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-4stdf
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290296701"
    uid: a7346740-7abe-431d-a181-236b288c032e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-15.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9n6sk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-145-15.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-9n6sk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:25:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:25:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://9279fd739d53822c722d1b72c7ac123f7cf5e8e9663a6c43c29d25241338ca16
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:25:02Z"
    hostIP: 10.0.145.15
    phase: Running
    podIP: 10.0.145.15
    podIPs:
    - ip: 10.0.145.15
    qosClass: BestEffort
    startTime: "2022-06-21T17:24:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:15:14Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-5cbb8
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290859141"
    uid: 9dd1a5a7-bbc1-425b-8fbf-94f5724c3c76
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-147-79.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-26vbg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-147-79.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-26vbg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://03f1f2f47ee17a2407d5a786b08e6b28a203e9574cbae2b6b896b5ed991d8b1f
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:15:24Z"
    hostIP: 10.0.147.79
    phase: Running
    podIP: 10.0.147.79
    podIPs:
    - ip: 10.0.147.79
    qosClass: BestEffort
    startTime: "2022-06-21T20:15:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:21:08Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-5l972
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290883684"
    uid: f568941a-20c2-4e93-b94c-99509399b19a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-110.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zpbvm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-130-110.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-zpbvm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://880ac9d01d7aab15d973b1c8ca6e99428e9358773e14b9cd41a06e4984913d34
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:21:16Z"
    hostIP: 10.0.130.110
    phase: Running
    podIP: 10.0.130.110
    podIPs:
    - ip: 10.0.130.110
    qosClass: BestEffort
    startTime: "2022-06-21T20:21:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:34:14Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-6q6qg
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290929849"
    uid: f2b3dcbd-c37d-4c82-80b9-bc9666b49cf4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-245.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8jgkc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-134-245.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-8jgkc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a940778facac90eb749e9a4c8889bf58cdb2cfdd4512783764799132a65b3810
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:34:19Z"
    hostIP: 10.0.134.245
    phase: Running
    podIP: 10.0.134.245
    podIPs:
    - ip: 10.0.134.245
    qosClass: BestEffort
    startTime: "2022-06-21T20:34:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:27:18Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-84wl4
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290308404"
    uid: 6e3da8ee-6e84-4332-a5f3-0264c3606138
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-150-201.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fk4cq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-150-201.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-fk4cq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://7281596a25e53c7d460a5ebd65a25539bbd10815af58fdbcf34eb3f7c9b923b8
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:27:27Z"
    hostIP: 10.0.150.201
    phase: Running
    podIP: 10.0.150.201
    podIPs:
    - ip: 10.0.150.201
    qosClass: BestEffort
    startTime: "2022-06-21T17:27:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:42:43Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-b6v49
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290955368"
    uid: 4a0c2c2e-98bd-42fe-9e89-80454be2056b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-156-105.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cfxp9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-156-105.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-cfxp9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://53b4491a60e26a5fb68736781f18d5352a93fb97fd179b9457693da50e07abc0
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:42:51Z"
    hostIP: 10.0.156.105
    phase: Running
    podIP: 10.0.156.105
    podIPs:
    - ip: 10.0.156.105
    qosClass: BestEffort
    startTime: "2022-06-21T20:42:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:08:26Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-ch82h
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290957607"
    uid: ed09e46a-794c-4b8f-8af1-cc47596d7954
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-139-131.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mbhgg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-139-131.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-mbhgg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:43:48Z"
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://d3647bd68cddcb73f345ba7f13803124014ee509783aca41a7b616e230aa16a6
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:08:34Z"
    hostIP: 10.0.139.131
    phase: Running
    podIP: 10.0.139.131
    podIPs:
    - ip: 10.0.139.131
    qosClass: BestEffort
    startTime: "2022-06-21T20:08:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:22:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-dbbhl
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290890029"
    uid: bdd584b2-0c2b-4649-970d-fe1b83a1d0f3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-3.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jvb5d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-130-3.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-jvb5d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8d8f18a1a7dc977a070ffef748cf11f787658ea6bfcf3450a1375767430fabb9
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:22:57Z"
    hostIP: 10.0.130.3
    phase: Running
    podIP: 10.0.130.3
    podIPs:
    - ip: 10.0.130.3
    qosClass: BestEffort
    startTime: "2022-06-21T20:22:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T06:09:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-f5zdw
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3288549245"
    uid: 3a6ed347-f8ed-4e8c-8578-da52c01976b7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-249.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9vwgq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-145-249.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-9vwgq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5dc04a2603ebf4fa19944182df5ec852ed079ac0f2d1a5cbe39a17c1c51da086
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T06:09:57Z"
    hostIP: 10.0.145.249
    phase: Running
    podIP: 10.0.145.249
    podIPs:
    - ip: 10.0.145.249
    qosClass: BestEffort
    startTime: "2022-06-21T06:09:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:22:58Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-g5nq2
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290890659"
    uid: 0f90f897-aa0a-4d6f-b9ac-4960d4549097
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-152.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4vx6s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-133-152.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-4vx6s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:23:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:23:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8a880d719fab797efe6225a1a730cbab9bd8d4185d12bc94f3b7b2d977f4d8d0
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:23:06Z"
    hostIP: 10.0.133.152
    phase: Running
    podIP: 10.0.133.152
    podIPs:
    - ip: 10.0.133.152
    qosClass: BestEffort
    startTime: "2022-06-21T20:22:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-14T04:30:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-ht72j
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3262035299"
    uid: 97b992cf-b85d-4f29-a2e4-35cb3d82b183
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-81.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nhs9q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-140-81.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-nhs9q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://439b7f7a02e4a897950ad6a9bbe24abf792088784bc548402f46e7c905b6b8ab
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:30:53Z"
    hostIP: 10.0.140.81
    phase: Running
    podIP: 10.0.140.81
    podIPs:
    - ip: 10.0.140.81
    qosClass: BestEffort
    startTime: "2022-06-14T04:30:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:11:49Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-j4pch
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290837255"
    uid: 0957b66c-01d2-46eb-a8dd-e40c70828196
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-57.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cpzs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-130-57.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-5cpzs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://47dcf5837f98bc7e04bd14b61e32da9514fcd9a81f41b16e5ee2b8e62d79aab6
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:11:56Z"
    hostIP: 10.0.130.57
    phase: Running
    podIP: 10.0.130.57
    podIPs:
    - ip: 10.0.130.57
    qosClass: BestEffort
    startTime: "2022-06-21T20:11:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:37:37Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-jwf2w
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290938684"
    uid: ec44480a-552d-4e27-9a8c-9bb2ec4b6e43
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-149-140.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bzjd6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-149-140.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-bzjd6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://3871f29ec5573fb1810b5e3b58a0f91b7ba4a24d410fa2935f0456e6e5ad3de0
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:37:42Z"
    hostIP: 10.0.149.140
    phase: Running
    podIP: 10.0.149.140
    podIPs:
    - ip: 10.0.149.140
    qosClass: BestEffort
    startTime: "2022-06-21T20:37:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-14T04:30:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-pnqd6
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3262035289"
    uid: e232ab42-f0ce-41ff-af07-d4b71dd624c8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-159-123.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nmzjb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-159-123.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-nmzjb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://b8e854f8fa382820483d2813bc636272fd8c427749328e9a88dfe89cd4985cde
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:30:53Z"
    hostIP: 10.0.159.123
    phase: Running
    podIP: 10.0.159.123
    podIPs:
    - ip: 10.0.159.123
    qosClass: BestEffort
    startTime: "2022-06-14T04:30:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:40:19Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-qwwjf
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290365654"
    uid: d1c197ad-caf9-4932-9299-98b321359a50
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-129-92.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9fsfk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-129-92.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-9fsfk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://71554f3eb77f862d93c39e31983c2a90fcb803c14bc91cec3d1cc039bab1a2d7
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:40:26Z"
    hostIP: 10.0.129.92
    phase: Running
    podIP: 10.0.129.92
    podIPs:
    - ip: 10.0.129.92
    qosClass: BestEffort
    startTime: "2022-06-21T17:40:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-14T04:30:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-r84x9
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3262035279"
    uid: 3b80065c-0c81-4b8b-92af-7ebfd4ec33c3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-231.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xtvcl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-133-231.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-xtvcl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://86e955c13276f259aa17fd254baaf911d5d41b2bc813e091708a09d8a441019c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:30:53Z"
    hostIP: 10.0.133.231
    phase: Running
    podIP: 10.0.133.231
    podIPs:
    - ip: 10.0.133.231
    qosClass: BestEffort
    startTime: "2022-06-14T04:30:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T16:58:12Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-rl7jn
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290198760"
    uid: 4d957a89-bda9-4d7b-b09e-40e6790f7412
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-136-253.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cvm8t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-136-253.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-cvm8t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:58:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://1acfdee50f198a65dd2994cdac75ef740a64a0d3b5cba3683fb60d0de0ed7e1c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T16:58:21Z"
    hostIP: 10.0.136.253
    phase: Running
    podIP: 10.0.136.253
    podIPs:
    - ip: 10.0.136.253
    qosClass: BestEffort
    startTime: "2022-06-21T16:58:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-14T04:30:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-rxzdr
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3262035293"
    uid: 97c235ef-5e1e-4f30-bec0-718c3fd065ae
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-169-113.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4gcnn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-169-113.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-4gcnn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8bde627208d65771641cb8bd072b3c4431716ebe64c5045199f5bcd64ec382ba
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:30:53Z"
    hostIP: 10.0.169.113
    phase: Running
    podIP: 10.0.169.113
    podIPs:
    - ip: 10.0.169.113
    qosClass: BestEffort
    startTime: "2022-06-14T04:30:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:20:30Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-v2hr2
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290880371"
    uid: 1b498bf8-d4d3-4c44-b577-b197c4a38795
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-141-152.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-74dc8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-141-152.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-74dc8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://6e9bf1d436f2bb46bcba9f20070d16b23bfb894ef667de43b0f0f3cf0a16af07
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:20:37Z"
    hostIP: 10.0.141.152
    phase: Running
    podIP: 10.0.141.152
    podIPs:
    - ip: 10.0.141.152
    qosClass: BestEffort
    startTime: "2022-06-21T20:20:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-14T04:30:52Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-w2t4g
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3262035268"
    uid: dd1a20ef-9238-4ad4-a076-3bcfff09d580
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-175-171.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cvkp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-175-171.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-5cvkp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-14T04:30:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://4df64bf9a535dfced3df91a7b8c11c1b56f3def992ace3173518d433746a4972
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-14T04:30:53Z"
    hostIP: 10.0.175.171
    phase: Running
    podIP: 10.0.175.171
    podIPs:
    - ip: 10.0.175.171
    qosClass: BestEffort
    startTime: "2022-06-14T04:30:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:33:15Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-wccqp
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290340234"
    uid: 0e949ec5-e749-4b38-b544-459718b7a0fc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-143.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6ktbx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-134-143.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-6ktbx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://7b7c67c78e5bc6d855fab1ea6bd860679d158a644ead9ab4a47f4ffcf1047f9a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:33:20Z"
    hostIP: 10.0.134.143
    phase: Running
    podIP: 10.0.134.143
    podIPs:
    - ip: 10.0.134.143
    qosClass: BestEffort
    startTime: "2022-06-21T17:33:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:20:31Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-wjgnt
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3290957314"
    uid: 4a0c2dc3-07ed-4576-aa66-f67fd7b83fb4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-142-124.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qhcxv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-142-124.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-qhcxv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:43:43Z"
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://f88a076963b1c13d252fd0f1ac28350cb9b5fc0dc8439ac6ff8875104275ded2
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:20:37Z"
    hostIP: 10.0.142.124
    phase: Running
    podIP: 10.0.142.124
    podIPs:
    - ip: 10.0.142.124
    qosClass: BestEffort
    startTime: "2022-06-21T20:20:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T12:14:07Z"
    generateName: dumper-
    labels:
      app: dumper
      controller-revision-hash: 65d8bb4f65
      pod-template-generation: "1"
    name: dumper-z5nnn
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dumper
      uid: f4500066-c797-4bfc-bdac-63d203b5b5df
    resourceVersion: "3289356975"
    uid: b3251289-3e6f-4e3b-bca2-df9a5ce4153d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-230.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        curler_container_id=$(chroot /host crictl ps --label=io.kubernetes.container.name=curl --label=io.kubernetes.pod.namespace=openshift-dns | awk 'NR==2{print $1}')
        curler_pod_ip_addr=$(chroot /host crictl inspect -o go-template --template='{{index .info.runtimeSpec.annotations "io.kubernetes.cri-o.IP.0"}}' "$curler_container_id")
        /sbin/tcpdump -i any "host $curler_pod_ip_addr and (udp port 53 or tcp port 53 or udp port 5353 or tcp port 5353)"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imagePullPolicy: IfNotPresent
      name: tcpdump
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        name: host-slash
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nrwb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: default-dockercfg-kc7dq
    nodeName: ip-10-0-140-230.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-builds-worker
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/ci-tests-worker
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host-slash
    - name: kube-api-access-9nrwb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:14:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ffc9af52f86fdd1956490b82c99f95cfd548d65c79616c5d5c1518dfe6e05bb8
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3e630fcf3b3a8c3b78e6766eb1e71db69a9ccdae9014e32464390806e74eaca9
      lastState: {}
      name: tcpdump
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:14:14Z"
    hostIP: 10.0.140.230
    phase: Running
    podIP: 10.0.140.230
    podIPs:
    - ip: 10.0.140.230
    qosClass: BestEffort
    startTime: "2022-06-21T12:14:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:55Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-24v6p
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245659732"
    uid: 08085d4c-10ba-4806-a202-4124d300d731
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-216.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7j28n
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-140-216.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-7j28n
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:51:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:51:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://fa2c0d21d1c3630337550c4ba713fbff08f6666f639d1c8a673aab3ff32b0ef0
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:51:17Z"
    hostIP: 10.0.140.216
    phase: Running
    podIP: 10.0.140.216
    podIPs:
    - ip: 10.0.140.216
    qosClass: Burstable
    startTime: "2022-06-04T08:04:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T12:53:13Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-2bpcg
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3289464867"
    uid: 95c221e0-d22b-46c1-b4ca-232e6ec60e26
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-59.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jgjvr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-134-59.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-jgjvr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://da16e2ad23a40d351a517e0cd7fc327fd4de333c7093f65ef6c12c252d359e17
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:53:23Z"
    hostIP: 10.0.134.59
    phase: Running
    podIP: 10.0.134.59
    podIPs:
    - ip: 10.0.134.59
    qosClass: Burstable
    startTime: "2022-06-21T12:53:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:37:17Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-42mz2
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290937762"
    uid: d9e8f767-b431-46b5-afee-06cd25231455
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-149-140.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4jfb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-149-140.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-w4jfb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:37:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ba77e693c5cbb4205c7586e0f55fcb4f68bc4f245bc89ea176b42f7fbbddca07
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:37:25Z"
    hostIP: 10.0.149.140
    phase: Running
    podIP: 10.0.149.140
    podIPs:
    - ip: 10.0.149.140
    qosClass: Burstable
    startTime: "2022-06-21T20:37:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:11:40Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-544ql
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290835933"
    uid: 45e006fa-4936-4bbd-adbc-26b61f82b963
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-57.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2ppdq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-130-57.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-2ppdq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:11:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://98a746a6bedee8cbd845eaf6454712b625d2e10fb098ed0151cf9522481f433d
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:11:47Z"
    hostIP: 10.0.130.57
    phase: Running
    podIP: 10.0.130.57
    podIPs:
    - ip: 10.0.130.57
    qosClass: Burstable
    startTime: "2022-06-21T20:11:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:20:21Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-5hqqh
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290879506"
    uid: 188c174a-0888-4b5d-9133-42f69514047f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-141-152.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4b5m7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-141-152.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-4b5m7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://0205e8d3794bce9e0a848855d9bf9aab480c8b148c98c7045986ab5a57c5939c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:20:29Z"
    hostIP: 10.0.141.152
    phase: Running
    podIP: 10.0.141.152
    podIPs:
    - ip: 10.0.141.152
    qosClass: Burstable
    startTime: "2022-06-21T20:20:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:15:05Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-6znw7
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290857179"
    uid: a6787ec8-48fa-446f-8484-35aadee015a7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-147-79.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f5p9c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-147-79.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-f5p9c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:15:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e8e70c198d2b1fb3494d6aee6beb895d7cd36e841480a02fafdb3f824f6aa6ca
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:15:12Z"
    hostIP: 10.0.147.79
    phase: Running
    podIP: 10.0.147.79
    podIPs:
    - ip: 10.0.147.79
    qosClass: Burstable
    startTime: "2022-06-21T20:15:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T19:18:26Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-72lq7
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290656141"
    uid: 19255088-460e-4756-8ae2-672dab4224f4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-138-5.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qczqs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-138-5.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-qczqs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T19:18:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T19:18:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T19:18:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T19:18:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://d6c7440e43bb4376d02666196b9739a407962983eba6ec14180de78db614990c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T19:18:35Z"
    hostIP: 10.0.138.5
    phase: Running
    podIP: 10.0.138.5
    podIPs:
    - ip: 10.0.138.5
    qosClass: Burstable
    startTime: "2022-06-21T19:18:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T12:13:46Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-72m4z
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3289356081"
    uid: f8ec7a5c-9737-4686-a54d-bd192c30a7bb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-230.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tzdrd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-140-230.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-tzdrd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:13:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:13:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:13:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:13:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://660cae5ac91e0889ad55fce76816cc63793d54fc8e94a20f72a38daf0775e35d
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:13:56Z"
    hostIP: 10.0.140.230
    phase: Running
    podIP: 10.0.140.230
    podIPs:
    - ip: 10.0.140.230
    qosClass: Burstable
    startTime: "2022-06-21T12:13:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T12:53:08Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-7bltj
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3289464347"
    uid: 30476f4c-26a3-4e81-9ebc-a5acd0ca3d50
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-132-21.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kbdgf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-132-21.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-kbdgf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a1e7855ad8048d5347d21faed3bcc6fc5cc4dd80787063c2cf799e79ad2383d5
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:53:16Z"
    hostIP: 10.0.132.21
    phase: Running
    podIP: 10.0.132.21
    podIPs:
    - ip: 10.0.132.21
    qosClass: Burstable
    startTime: "2022-06-21T12:53:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T18:05:07Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-7r4st
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290447375"
    uid: e39f2f7d-01cc-4b6b-aa03-0b63a2b9657d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-128-159.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ppm6h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-128-159.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-ppm6h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:05:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:05:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:05:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:05:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://2ab23a99458de1fb242302182d9b8b7439453bf1f6c69820a07ca09bcb79aff2
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T18:05:15Z"
    hostIP: 10.0.128.159
    phase: Running
    podIP: 10.0.128.159
    podIPs:
    - ip: 10.0.128.159
    qosClass: Burstable
    startTime: "2022-06-21T18:05:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T16:48:10Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-84bjf
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290167710"
    uid: 3c2969f6-c64b-409d-9ff0-e7848bea31bd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-136-73.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nsftk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-136-73.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-nsftk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:48:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:48:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:48:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:48:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e0d9d0379130b05be777e27914cf9ca57713b2b67314e257d43e8b9c0cc2b9fa
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T16:48:18Z"
    hostIP: 10.0.136.73
    phase: Running
    podIP: 10.0.136.73
    podIPs:
    - ip: 10.0.136.73
    qosClass: Burstable
    startTime: "2022-06-21T16:48:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:24:33Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-88ngz
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290295315"
    uid: 298c6b34-e8b2-4b62-abae-341c368b2a41
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-15.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-42bhv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-145-15.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-42bhv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:24:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ce9362a0d14c4b60ab5e5598344bc44c21109a23d929e08e46d2bb3323d5f132
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:24:41Z"
    hostIP: 10.0.145.15
    phase: Running
    podIP: 10.0.145.15
    podIPs:
    - ip: 10.0.145.15
    qosClass: Burstable
    startTime: "2022-06-21T17:24:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-09T09:24:30Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-95flh
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245937899"
    uid: 341ed0c7-f892-4506-8083-2723b191f06b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-124.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n2qzc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-145-124.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-n2qzc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T09:24:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T09:24:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T09:24:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T09:24:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://7ed6f6dbd216ac9865104b1f3159e01d013ac66445a7171938b215f5c670d226
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-09T09:24:37Z"
    hostIP: 10.0.145.124
    phase: Running
    podIP: 10.0.145.124
    podIPs:
    - ip: 10.0.145.124
    qosClass: Burstable
    startTime: "2022-06-09T09:24:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T12:53:07Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-9b8x2
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3289464280"
    uid: d7e1294d-fdba-4a3a-9072-094dee64610e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-138-242.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wtj6r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-138-242.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-wtj6r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a16391344d5512fb667d7759813e363be1e8a826a7f6dd2e77e9bdb6545dcaa4
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:53:15Z"
    hostIP: 10.0.138.242
    phase: Running
    podIP: 10.0.138.242
    podIPs:
    - ip: 10.0.138.242
    qosClass: Burstable
    startTime: "2022-06-21T12:53:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:05:01Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-9hhj4
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245657470"
    uid: 21f6dbf2-0f57-4e3d-b6cb-af791ab59f2a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-139-92.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kg27w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-139-92.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-kg27w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:05:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:50:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:50:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:05:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8479e7fee22d9ec016af5b61ff3c603cb27dcc2ec3849bfdd61f0017aefd31e6
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:50:44Z"
    hostIP: 10.0.139.92
    phase: Running
    podIP: 10.0.139.92
    podIPs:
    - ip: 10.0.139.92
    qosClass: Burstable
    startTime: "2022-06-04T08:05:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T16:57:52Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-9j6jn
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290195895"
    uid: b10c1480-3ab9-4706-bbd9-7778754fdce9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-136-253.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4jcpv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-136-253.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-4jcpv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:57:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:57:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:57:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T16:57:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ea4b00b63f4d89c4dba5ff5e8c8a8b4a79a5ab528955f1428860a819853d1a56
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T16:57:59Z"
    hostIP: 10.0.136.253
    phase: Running
    podIP: 10.0.136.253
    podIPs:
    - ip: 10.0.136.253
    qosClass: Burstable
    startTime: "2022-06-21T16:57:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:58Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-9p9z2
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245667304"
    uid: 139ef2e2-4f7d-4d0b-bec3-4d35d92741d7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-169-113.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5kzzk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-169-113.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-5kzzk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:53:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:53:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://635628544a63777e3bffe5aa278791365a9a528ce2c7d2008706b491ec306581
      image: image-registry.openshift-image-registry.svc:5000/ci-ln-6wz80kk/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: image-registry.openshift-image-registry.svc:5000/ci-ln-6wz80kk/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:53:19Z"
    hostIP: 10.0.169.113
    phase: Running
    podIP: 10.0.169.113
    podIPs:
    - ip: 10.0.169.113
    qosClass: Burstable
    startTime: "2022-06-04T08:04:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:42:23Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-9pvjp
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290954569"
    uid: 6937e035-7c3d-4e43-9e1a-74de8f7b7cb5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-156-105.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jh54q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-156-105.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-jh54q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:42:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://6c62d41cfd52cd69f5ce834ef959b505160fe6f7bc539a8b4eab9ac746bb5734
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:42:31Z"
    hostIP: 10.0.156.105
    phase: Running
    podIP: 10.0.156.105
    podIPs:
    - ip: 10.0.156.105
    qosClass: Burstable
    startTime: "2022-06-21T20:42:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T18:35:47Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-bjsjh
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290535256"
    uid: 1c7e419c-58b1-4e71-a2d3-be97a571962c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-136-19.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9sv4f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-136-19.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-9sv4f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:35:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:35:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:35:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:35:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://da7b157e82a8347f5461d4e01461e32bcacb46ceb33abfecd92053174f8d45a1
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T18:35:55Z"
    hostIP: 10.0.136.19
    phase: Running
    podIP: 10.0.136.19
    podIPs:
    - ip: 10.0.136.19
    qosClass: Burstable
    startTime: "2022-06-21T18:35:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:20:20Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-bkxcr
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290957300"
    uid: 70209b5d-4ad5-466d-b9c3-a5bf88bc82c5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-142-124.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j58cp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-142-124.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-j58cp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:43:43Z"
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://9b3114589ef19e91863f133b77e3cf87e756200d05884d978c5794dae293d904
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:20:28Z"
    hostIP: 10.0.142.124
    phase: Running
    podIP: 10.0.142.124
    podIPs:
    - ip: 10.0.142.124
    qosClass: Burstable
    startTime: "2022-06-21T20:20:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:26:58Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-ccwn4
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290306445"
    uid: 2d9619f0-b3ba-4dc0-a8e8-2b8adc95224d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-150-201.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q4p2z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-150-201.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-q4p2z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:26:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:27:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:26:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://76f4c4c3e82c0da2d909c7e367732e9ba767ee3b8454f0d87c0828b6ab5e45af
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:27:09Z"
    hostIP: 10.0.150.201
    phase: Running
    podIP: 10.0.150.201
    podIPs:
    - ip: 10.0.150.201
    qosClass: Burstable
    startTime: "2022-06-21T17:26:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:59Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-cdlv7
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245658098"
    uid: a053508a-0a5b-4e25-857d-97c7e729790d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-143-66.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2jfxg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-143-66.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-2jfxg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:50:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:50:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://c12648488d99a85b47e58e7a1f52b37b20eeccdd7be2f0c0fe5ff35d55b8b203
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:50:55Z"
    hostIP: 10.0.143.66
    phase: Running
    podIP: 10.0.143.66
    podIPs:
    - ip: 10.0.143.66
    qosClass: Burstable
    startTime: "2022-06-04T08:04:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:22:48Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-cz24b
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290889968"
    uid: f1fdc4b0-b538-41d0-917e-e35d022f31f3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-152.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m9hw2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-133-152.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-m9hw2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://1f3c6a81f0035f9e17d18799db2d7b80fdebb3514bde1c1d507d8b37e2469020
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:22:56Z"
    hostIP: 10.0.133.152
    phase: Running
    podIP: 10.0.133.152
    podIPs:
    - ip: 10.0.133.152
    qosClass: Burstable
    startTime: "2022-06-21T20:22:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:22:42Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-fptbv
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290889223"
    uid: bf6ad4ec-0bb9-4db3-8ff1-c413f38593ea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-3.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f8bx4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-130-3.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-f8bx4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:22:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://6efd274c7bf3f1358844b09eaf1571a7ca6b5592716cea660e25f17286707aae
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:22:50Z"
    hostIP: 10.0.130.3
    phase: Running
    podIP: 10.0.130.3
    podIPs:
    - ip: 10.0.130.3
    qosClass: Burstable
    startTime: "2022-06-21T20:22:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T18:20:31Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-hvtw8
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290492113"
    uid: 0edd61b6-5c4c-4955-909d-a7c4a29c3816
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-219.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p45vp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-130-219.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-p45vp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:20:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:20:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:20:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:20:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5ae0e8d344fb270f5d1e4a1732e9f9b86a13dd462cdf0fd4ae85dca5cd99716e
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T18:20:39Z"
    hostIP: 10.0.130.219
    phase: Running
    podIP: 10.0.130.219
    podIPs:
    - ip: 10.0.130.219
    qosClass: Burstable
    startTime: "2022-06-21T18:20:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:34:04Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-j9xzc
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290929510"
    uid: e2d4e138-94b0-4389-8418-d6979f3dc9bd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-245.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8cvc7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-134-245.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-8cvc7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:34:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://367b3690c7bdad6fe792d51175555956038e3bbe2b74b92c3575eea0df9452ed
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:34:12Z"
    hostIP: 10.0.134.245
    phase: Running
    podIP: 10.0.134.245
    podIPs:
    - ip: 10.0.134.245
    qosClass: Burstable
    startTime: "2022-06-21T20:34:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:20:58Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-jw4h8
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290882384"
    uid: 0ffd6992-8674-4b8a-a29e-f5aabc7281b4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-110.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7tc5w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-130-110.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-7tc5w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:21:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:20:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://1ff7edcb42de84c4e798e686b2def8e96ed95d48115bbe14a50615ce00b9ec5f
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:21:06Z"
    hostIP: 10.0.130.110
    phase: Running
    podIP: 10.0.130.110
    podIPs:
    - ip: 10.0.130.110
    qosClass: Burstable
    startTime: "2022-06-21T20:20:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:05:01Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-kbvg5
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245675583"
    uid: 816f9fe4-c872-4cb3-8711-d996ec8a9a0d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-141.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k7dtm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-130-141.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-k7dtm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:05:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:56:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:56:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:05:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://b1b0f163cdbe69d6a1367bf00e15c6484296781487b6509a41528d33b012b002
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:56:20Z"
    hostIP: 10.0.130.141
    phase: Running
    podIP: 10.0.130.141
    podIPs:
    - ip: 10.0.130.141
    qosClass: Burstable
    startTime: "2022-06-04T08:05:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T18:16:54Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-m2kgs
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290485396"
    uid: a962011b-c881-4695-a2cb-054a13e1e6a7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-130-1.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b2qlk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-130-1.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-b2qlk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:16:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:17:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:17:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T18:16:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e308ee5165f6018c0b3c494c5d1cbfcab255c164b1ed3e6c518c6335441abf8a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T18:17:03Z"
    hostIP: 10.0.130.1
    phase: Running
    podIP: 10.0.130.1
    podIPs:
    - ip: 10.0.130.1
    qosClass: Burstable
    startTime: "2022-06-21T18:16:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:46:29Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-nd87s
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290386628"
    uid: da00117a-bb4a-4852-aea3-9db0f5de77ab
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-143-86.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-skptl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-143-86.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-skptl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:46:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:46:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:46:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:46:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://a7a3a010335ee8bd7f9f1b43c1ee22097082c5ccdbe692782b1a4941d51f9dbf
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:46:37Z"
    hostIP: 10.0.143.86
    phase: Running
    podIP: 10.0.143.86
    podIPs:
    - ip: 10.0.143.86
    qosClass: Burstable
    startTime: "2022-06-21T17:46:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T12:53:02Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-psdh4
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3289463802"
    uid: 97d8dcbb-65f2-41ce-9265-46d9e6b1f2a9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-136-212.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vsfnj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-136-212.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-vsfnj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T12:53:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://db23e0c7004ba14f16cfc3839e1f385edd2800d6a241496d6e268d67e1747cf2
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T12:53:10Z"
    hostIP: 10.0.136.212
    phase: Running
    podIP: 10.0.136.212
    podIPs:
    - ip: 10.0.136.212
    qosClass: Burstable
    startTime: "2022-06-21T12:53:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:39:38Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-q6smk
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290362657"
    uid: 1e7b0f2b-154a-4bc1-80d8-3eb6f4dfe702
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-131-130.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v9vbj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-131-130.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-v9vbj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:39:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:39:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:39:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:39:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://43a606d796f7d47fca42409434a2ac91b2dea7a0ecf0dc373e3037b7fc0e1719
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:39:46Z"
    hostIP: 10.0.131.130
    phase: Running
    podIP: 10.0.131.130
    podIPs:
    - ip: 10.0.131.130
    qosClass: Burstable
    startTime: "2022-06-21T17:39:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T06:09:42Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-r9l8g
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3288548814"
    uid: 8e3e0c2e-aea5-4e76-a7be-1fb2bd937bd2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-145-249.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d7q8l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-145-249.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-d7q8l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T06:09:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e9186342d48266b9f00edd1d99a5e3e6d93793b8d1d36cda89a632ff740cf577
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T06:09:50Z"
    hostIP: 10.0.145.249
    phase: Running
    podIP: 10.0.145.249
    podIPs:
    - ip: 10.0.145.249
    qosClass: Burstable
    startTime: "2022-06-21T06:09:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:32:55Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-rdfk6
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290338282"
    uid: d4370e96-22df-4550-aaf2-1370313c55a4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-143.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zflxk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-134-143.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-zflxk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:32:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:33:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:32:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e089cc15096e8709ee9db602d6241bbc7afc438300f126cd19a6b69e3652fa90
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:33:03Z"
    hostIP: 10.0.134.143
    phase: Running
    podIP: 10.0.134.143
    podIPs:
    - ip: 10.0.134.143
    qosClass: Burstable
    startTime: "2022-06-21T17:32:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:59Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-sm6k8
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245969537"
    uid: 0938ef9d-be1e-47a7-890f-d7e5226516e3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-134-253.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jrlwr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-134-253.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-jrlwr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T09:38:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T09:38:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e1875b0503d87f69b2c3904a859dda24e9ea5198259ad90ae6383ab45e4f601e
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T09:38:44Z"
    hostIP: 10.0.134.253
    phase: Running
    podIP: 10.0.134.253
    podIPs:
    - ip: 10.0.134.253
    qosClass: Burstable
    startTime: "2022-06-04T08:04:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:39:59Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-t4gkw
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290364015"
    uid: f57990d8-b3cf-4778-8def-3fa7f01b1951
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-129-92.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xbbm6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-129-92.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-xbbm6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:39:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:40:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:39:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5f8519e9a45731f7144953b5f17fb55464a3572bfe83672bc72afdea726ef467
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:40:06Z"
    hostIP: 10.0.129.92
    phase: Running
    podIP: 10.0.129.92
    podIPs:
    - ip: 10.0.129.92
    qosClass: Burstable
    startTime: "2022-06-21T17:39:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:55Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-tmm4x
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245696450"
    uid: c0d58d85-39e8-4b13-93be-c28622514468
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-133-231.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nhhhq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-133-231.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-nhhhq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T08:01:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T08:01:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://6798582f7bed27ee5c68adc3dd396f8ec89bac284eaec3a5723ef56d6861d38c
      image: image-registry.openshift-image-registry.svc:5000/ci-op-fm9zghv2/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: image-registry.openshift-image-registry.svc:5000/ci-op-fm9zghv2/stable@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T08:01:19Z"
    hostIP: 10.0.133.231
    phase: Running
    podIP: 10.0.133.231
    podIPs:
    - ip: 10.0.133.231
    qosClass: Burstable
    startTime: "2022-06-04T08:04:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:56Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-trmtf
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245684395"
    uid: 92f16150-6262-43f4-b039-b6d63dec9d47
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-175-171.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zmrn7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-175-171.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-zmrn7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:58:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:58:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8773c1942ebe07fdc4ae7616a002bd0bee65931bc8abf3da8e1303ae728ed52f
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:58:14Z"
    hostIP: 10.0.175.171
    phase: Running
    podIP: 10.0.175.171
    podIPs:
    - ip: 10.0.175.171
    qosClass: Burstable
    startTime: "2022-06-04T08:04:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:04:58Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-w8cbg
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245664601"
    uid: 87b9be66-dfa9-40d0-8d08-13fddde0cb5c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-159-123.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8bj2w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-159-123.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-8bj2w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:52:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T07:52:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:04:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://f1f3fa937737179b415b0470570524f922623b58fef42aa8a059153ef21a7d5d
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T07:52:50Z"
    hostIP: 10.0.159.123
    phase: Running
    podIP: 10.0.159.123
    podIPs:
    - ip: 10.0.159.123
    qosClass: Burstable
    startTime: "2022-06-04T08:04:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T20:08:16Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-x62kr
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290957626"
    uid: 548ab74a-43b1-4ca7-a36b-df3711e0d5d9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-139-131.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zj6st
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-139-131.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-zj6st
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:43:48Z"
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T20:08:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5ed9f9d356c6fea4c8b70e2aedb5ea27f346e6dd2ad09bb23c33b40f093c05a2
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T20:08:23Z"
    hostIP: 10.0.139.131
    phase: Running
    podIP: 10.0.139.131
    podIPs:
    - ip: 10.0.139.131
    qosClass: Burstable
    startTime: "2022-06-21T20:08:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-21T17:44:14Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-zfdlv
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3290380358"
    uid: 0dcf00fa-225d-4a0c-a879-3fe5aa458b6a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-137-156.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z9ctr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-137-156.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-z9ctr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:44:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:44:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:44:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-21T17:44:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://00c8cc444a92a4c6171871e3d35a63f5c1213bc1ce6bac6d06fb4de384674940
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-06-21T17:44:22Z"
    hostIP: 10.0.137.156
    phase: Running
    podIP: 10.0.137.156
    podIPs:
    - ip: 10.0.137.156
    qosClass: Burstable
    startTime: "2022-06-21T17:44:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2022-06-04T08:05:03Z"
    generateName: node-resolver-
    labels:
      controller-revision-hash: 64bbd5b5c8
      dns.operator.openshift.io/daemonset-node-resolver: ""
      pod-template-generation: "35"
    name: node-resolver-zjrxm
    namespace: openshift-dns
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-resolver
      uid: 64186ea4-cdbe-47db-a13e-fe4f1b7ec315
    resourceVersion: "3245708933"
    uid: 50ca3cb8-c1a5-430b-ba9e-c5392be83094
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-140-81.ec2.internal
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -uo pipefail

        trap 'jobs -p | xargs kill || true; wait; exit 0' TERM

        OPENSHIFT_MARKER="openshift-generated-node-resolver"
        HOSTS_FILE="/etc/hosts"
        TEMP_FILE="/etc/hosts.tmp"

        IFS=', ' read -r -a services <<< "${SERVICES}"

        # Make a temporary file with the old hosts file's attributes.
        cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"

        while true; do
          declare -A svc_ips
          for svc in "${services[@]}"; do
            # Fetch service IP from cluster dns if present. We make several tries
            # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
            # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
            # support UDP loadbalancers and require reaching DNS through TCP.
            cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t A +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"'
                  'dig -t AAAA +tcp +retry=0 @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"|grep -v "^;"')
            for i in ${!cmds[*]}
            do
              ips=($(eval "${cmds[i]}"))
              if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
                svc_ips["${svc}"]="${ips[@]}"
                break
              fi
            done
          done

          # Update /etc/hosts only if we get valid service IPs
          # We will not update /etc/hosts when there is coredns service outage or api unavailability
          # Stale entries could exist in /etc/hosts if the service is deleted
          if [[ -n "${svc_ips[*]-}" ]]; then
            # Build a new hosts file from /etc/hosts with our custom entries filtered out
            grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"

            # Append resolver entries for services
            for svc in "${!svc_ips[@]}"; do
              for ip in ${svc_ips[${svc}]}; do
                echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
              done
            done

            # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
            # Replace /etc/hosts with our modified version if needed
            cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
            # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
          fi
          sleep 60 & wait
          unset svc_ips
        done
      env:
      - name: SERVICES
        value: image-registry.openshift-image-registry.svc
      - name: NAMESERVER
        value: 172.30.0.10
      - name: CLUSTER_DOMAIN
        value: cluster.local
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imagePullPolicy: IfNotPresent
      name: dns-node-resolver
      resources:
        requests:
          cpu: 5m
          memory: 21Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/hosts
        name: hosts-file
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wwdps
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: node-resolver-dockercfg-cfkdq
    nodeName: ip-10-0-140-81.ec2.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: node-resolver
    serviceAccountName: node-resolver
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/hosts
        type: File
      name: hosts-file
    - name: kube-api-access-wwdps
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:05:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T08:04:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-06-09T08:04:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-06-04T08:05:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://2cbb5e864fe0e0af02daab523011c115a2b837adfc33d03d4af9a0df169ec39a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ba44dead03ea74107f90d58525106fb52d27a120b73c6cc8e2be31d37043ca1c
      lastState: {}
      name: dns-node-resolver
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2022-06-09T08:04:01Z"
    hostIP: 10.0.140.81
    phase: Running
    podIP: 10.0.140.81
    podIPs:
    - ip: 10.0.140.81
    qosClass: Burstable
    startTime: "2022-06-04T08:05:03Z"
kind: PodList
metadata:
  resourceVersion: "3290959246"
